@article{Marasco2022,
   abstract = {<p> Flexible and bendable electronics are gaining a lot of interest in these last years. In this scenario, compact antennas on flexible substrates represent a strategical technological step to pave the way to a new class of wearable systems. A crucial issue to overcome is represented by the poor radiation properties of compact antennas, especially in the case of flexible and thin substrates. In this paper, we propose an innovative design of a miniaturized evolved patch antenna whose radiation properties have been enhanced with a Split Ring Resonator (SRR) placed between the top and the ground plane. The antenna has been realized on a flexible and biocompatible substrate polyethylene naphthalate (PEN) of 250 μm by means of a new fabrication protocol that involves a three-layer 3D-inkjet printing and an alignment step. The antenna has been characterized in terms of the scattering parameter S <sub>11</sub> and the radiation pattern showing a good agreement between simulations and measurements. </p>},
   author = {I. Marasco and G. Niro and V. M. Mastronardi and F. Rizzi and A. D’Orazio and M. De Vittorio and M. Grande},
   doi = {10.1038/s41598-022-14447-9},
   issn = {2045-2322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   pages = {10327},
   title = {A compact evolved antenna for 5G communications},
   volume = {12},
   year = {2022},
}
@article{Tang1996,
   author = {K.S. Tang and K.F. Man and S. Kwong and Q. He},
   doi = {10.1109/79.543973},
   issn = {10535888},
   issue = {6},
   journal = {IEEE Signal Processing Magazine},
   pages = {22-37},
   title = {Genetic algorithms and their applications},
   volume = {13},
   year = {1996},
}

@misc{Zhonget.al,
  doi = {10.48550/ARXIV.2209.13077},
  
  author = {Zhong, Rui and Zhang, Enzhi and Munetomo, Masaharu},
  
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Accelerating the Genetic Algorithm for Large-scale Traveling Salesman Problems by Cooperative Coevolutionary Pointer Network with Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{Aizaz2016,
   author = {Zainab Aizaz and Poonam Sinha},
   doi = {10.1109/SCEECS.2016.7509263},
   isbn = {978-1-4673-7918-2},
   journal = {2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)},
   month = {3},
   pages = {1-6},
   publisher = {IEEE},
   title = {A survey of cognitive radio reconfigurable antenna design and proposed design using genetic algorithm},
   year = {2016},
}
@article{Goray2020,
   author = {Leonid I. Goray and Alexander S. Dashkov},
   doi = {10.1109/DD49902.2020.9274560},
   isbn = {978-1-6654-0456-3},
   journal = {2020 Days on Diffraction (DD)},
   month = {5},
   pages = {31-37},
   publisher = {IEEE},
   title = {GPU-based optimizations of the boundary integral equation method to solve direct and inverse diffraction grating problems},
   year = {2020},
}
@article{Nandini2020,
   author = {Nandini Av and Nilita Anil Kumar},
   doi = {10.1109/ICCCNT49239.2020.9225312},
   isbn = {978-1-7281-6851-7},
   journal = {2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)},
   month = {7},
   pages = {1-6},
   publisher = {IEEE},
   title = {Image Encryption Using Genetic Algorithm and Bit-Slice Rotation},
   year = {2020},
}
@article{Still2021,
   author = {Luisa Still and Marc Oispuu and Wolfgang Koch},
   doi = {10.23919/FUSION49465.2021.9626955},
   isbn = {978-1-7377497-1-4},
   journal = {2021 IEEE 24th International Conference on Information Fusion (FUSION)},
   month = {11},
   pages = {1-8},
   publisher = {IEEE},
   title = {Optimal Sensor Placement for Shooter Localization Using a Genetic Algorithm},
   year = {2021},
}
@article{Chen2008,
   author = {Ming Chen and Zhengwei Yao},
   doi = {10.1109/WGEC.2008.23},
   isbn = {978-0-7695-3334-6},
   journal = {2008 Second International Conference on Genetic and Evolutionary Computing},
   month = {9},
   pages = {115-119},
   publisher = {IEEE},
   title = {Classification Techniques of Neural Networks Using Improved Genetic Algorithms},
   year = {2008},
}

@article{Haupt1996,
  author={Haupt, R.L.},
  journal={1996 IEEE Aerospace Applications Conference. Proceedings}, 
  title={Genetic algorithm design of antenna arrays}, 
  year={1996},
  volume={1},
  number={},
  pages={103-109 vol.1},
  doi={10.1109/AERO.1996.495875}}
  
@article{Liu2019,
  author ={Donghai Liu},
  doi={10.3233/JCM-191019},
  journal={Journal of Computational Methods in Sciences and Engineering},
  month={8},
  year={2019},
  title={Mathematical modeling analysis of genetic algorithms under schema theorem},
  volume={9},
  pages={131-137}
}

@article{DODSON1976243,
    title = {Darwin's law of natural selection and Thom's theory of catastrophes},
    journal = {Mathematical Biosciences},
    volume = {28},
    number = {3},
    pages = {243-274},
    year = {1976},
    issn = {0025-5564},
    doi = {https://doi.org/10.1016/0025-5564(76)90127-9},
    author = {M.M. Dodson},
    abstract = {The fitness of a population is defined to be a real smooth function of its environment and phenotype. Darwin's law of natural selection implies that a population in equilibrium with its environment under natural selection will have a phenotype which maximizes the fitness locally. By using Thom theory it is possible under a number of assumptions to make qualitative inferences about the phenotypic change of a population subject to natural selection in a continuously varying environment. When the environment is one or two dimensional, Thom's catastrophe theorem implies that sudden and substantial changes in phenotype can only arise from the fold and cusp catastrophes. It is shown that a population in stable equilibrium with its environment exhibits genetic assimilation, and that the main modes of evolution can arise from appropriate variations in the environment.}
}

@article{Katoch2021,
   author = {Sourabh Katoch and Sumit Singh Chauhan and Vijay Kumar},
   doi = {10.1007/s11042-020-10139-6},
   issn = {1380-7501},
   issue = {5},
   journal = {Multimedia Tools and Applications},
   month = {2},
   pages = {8091-8126},
   title = {A review on genetic algorithm: past, present, and future},
   volume = {80},
   year = {2021},
}

@article{Holland,
 ISSN = {00368733, 19467087},
 author = {John H. Holland},
 journal = {Scientific American},
 number = {1},
 pages = {66--73},
 publisher = {Scientific American, a division of Nature America, Inc.},
 title = {Genetic Algorithms},
 urldate = {2022-11-12},
 volume = {267},
 year = {1992},
 doi = {10.2307/24939139}
}

@article{Jebari,
author = {Jebari, Khalid},
year = {2013},
month = {12},
pages = {333-344},
title = {Selection Methods for Genetic Algorithms},
volume = {3},
journal = {International Journal of Emerging Sciences}
}

@article{Hornby,
author = {Hornby, Gregory S. and Al Globus and Derek S. Linden and Jason D. Lohn},
year = {2006},
month = {9},
pages = {1-8},
title = {Automated antenna design with evolutionary algorithm},
journal = {American Institute of Aeronautics and Astronautics},
volume = {NASA}
}

@article{Yilei,
author = {Yilei He},
year = {2014},
title = {Shape Optimization of Airfoils Without and With Ground Effect Using a Multi-Objective Genetic Algorithm},
journal = {McKelvey School of Engineering Theses and Dissertations}
}


@article{Zhang2012,
   abstract = {<p>Targeting quasar candidates is always an important task for large spectroscopic sky survey projects. Astronomers never give up thinking out effective approaches to separate quasars from stars. The previous methods on this issue almost belong to supervised methods or color-color cut. In this work, we compare the performance of a supervised method – Support Vector Machine (SVM)– with that of an unsupervised method one-class SVM. The performance of SVM is better than that of one-class SVM. But one-class SVM is an unsupervised algorithm which is helpful to recognize rare or mysterious objects. Combining supervised methods with unsupervised methods is effective to improve the performance of a single classifier.</p>},
   author = {Yanxia Zhang and Yongheng Zhao and Hongwen Zheng and Xue-bing Wu},
   doi = {10.1017/S1743921312017176},
   issn = {1743-9213},
   issue = {S288},
   journal = {Proceedings of the International Astronomical Union},
   keywords = {Astronomical databases: miscellaneous,Catalogs,Classification,Methods: data analysis,Methods: statistical},
   month = {8},
   pages = {333-334},
   publisher = {Cambridge University Press},
   title = {Classification of Quasars and Stars by Supervised and Unsupervised Methods},
   volume = {8},
   year = {2012},
}
@article{Wierzbiński,
   abstract = {<p>The heavenly bodies are objects that swim in the outer space. The classification of these objects is a challenging task for astronomers. This article presents a novel methodology that enables an efficient and accurate classification of cosmic objects (3 classes) based on evolutionary optimization of classifiers. This research collected the data from Sloan Digital Sky Survey database. In this work, we are proposing to develop a novel machine learning model to classify stellar spectra of stars, quasars and galaxies. First, the input data are normalized and then subjected to principal component analysis to reduce the dimensionality. Then, the genetic algorithm is implemented on the data which helps to find the optimal parameters for the classifiers. We have used 21 classifiers to develop an accurate and robust classification with fivefold cross-validation strategy. Our developed model has achieved an improvement in the accuracy using nineteen out of twenty-one models. We have obtained the highest classification accuracy of 99.16\%, precision of 98.78\%, recall of 98.08\% and F1-score of 98.32\% using evolutionary system based on voting classifier. The developed machine learning prototype can help the astronomers to make accurate classification of heavenly bodies in the sky. Proposed evolutionary system can be used in other areas where accurate classification of many classes is required.</p>},
   author = {Michał Wierzbiński and Paweł Pławiak and Mohamed Hammad and U. Rajendra Acharya},
   doi = {10.1007/s00500-021-05687-4},
   issn = {1432-7643},
   issue = {10},
   journal = {Soft Computing},
   month = {5},
   pages = {7213-7228},
   title = {Development of accurate classification of heavenly bodies using novel machine learning techniques},
   volume = {25},
   year = {2021},
}

@article{Kim2016,
   abstract = {Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering.},
   author = {Edward J. Kim and Robert J. Brunner},
   doi = {10.1093/mnras/stw2672},
   month = {8},
   title = {Star-galaxy Classification Using Deep Convolutional Neural Networks},
   year = {2016},
}

@article{Cui,
title = {A new hyperparameters optimization method for convolutional neural networks},
journal = {Pattern Recognition Letters},
volume = {125},
pages = {828-834},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.02.009},
author = {Hua Cui and Jie Bai},
keywords = {Convolutional neural networks, Hyperparameters optimization, Multilevel evolutionary optimization, Bayesian optimization},
}

@article{BergstraBengio,
    author  = {James Bergstra and Yoshua Bengio},
    title   = {Random Search for Hyper-Parameter Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2012},
    volume  = {13},
    number  = {10},
    pages   = {281--305}
}

@article{Telikani,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
doi = {10.1145/3467477},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {161},
numpages = {35},
keywords = {Evolutionary computation, learning optimization, swarm intelligence}
}

@article{Young,
author = {Young, Steven R. and Rose, Derek C. and Karnowski, Thomas P. and Lim, Seung-Hwan and Patton, Robert M.},
title = {Optimizing Deep Learning Hyper-Parameters through an Evolutionary Algorithm},
year = {2015},
isbn = {9781450340069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2834892.2834896},
journal = {Proceedings of the Workshop on Machine Learning in High-Performance Computing Environments},
articleno = {4},
numpages = {5},
keywords = {convolutional neural networks, hyper-parameter optimization, evolutionary algorithm, deep learning},
location = {Austin, Texas},
series = {MLHPC '15}
}


@article{BergstraYamins,
  title = 	 {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
  author = 	 {Bergstra, James and Yamins, Daniel and Cox, David},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {115--123},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
}

@article{Khalifa,
  author={Khalifa, Nour Eldeen and Hamed Taha, Mohamed and Hassanien, Aboul Ella and Selim, Ibrahim},
  booktitle={2018 International Conference on Computing Sciences and Engineering (ICCSE)}, 
  title={Deep Galaxy V2: Robust Deep Convolutional Neural Networks for Galaxy Morphology Classifications}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICCSE1.2018.8374210}}

@article{Hubble,
       author = {{Hubble}, E.~P.},
        title = {Extragalactic nebulae.},
      journal = {ApJ},
         year = {1926},
        month = {12},
       volume = {64},
        pages = {321-369},
          doi = {10.1086/143018},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Philip,
	author = {{Philip, N. S.} and {Wadadekar, Y.} and {Kembhavi, A.} and {Joseph, K. B.}},
	title = {A difference boosting neural network for automated 
star-galaxy classification},
	DOI= "10.1051/0004-6361:20020219",
	journal = {Astronomy \& Astrophysics Journal},
	year = 2002,
	volume = 385,
	number = 3,
	pages = "1119-1126",
}

@article{Jin2019,
    author = {Jin, Xin and Zhang, Yanxia and Zhang, Jingyi and Zhao, Yongheng and Wu, Xue-bing and Fan, Dongwei},
    title = "{Efficient selection of quasar candidates based on optical and infrared photometric data using machine learning}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {485},
    number = {4},
    pages = {4539-4549},
    year = {2019},
    month = {03},
    issn = {0035-8711},
    doi = {10.1093/mnras/stz680},
    eprint = {https://academic.oup.com/mnras/article-pdf/485/4/4539/28227898/stz680.pdf},
}

@article{Becker2020,
    author = {Becker, I and Pichara, K and Catelan, M and Protopapas, P and Aguirre, C and Nikzat, F},
    title = "{Scalable end-to-end recurrent neural network for variable star classification}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    volume = {493},
    number = {2},
    pages = {2981-2995},
    year = {2020},
    month = {02},
    issn = {0035-8711},
    url = {\url{https://doi.org/10.1093/mnras/staa350}},
    eprint = {https://academic.oup.com/mnras/article-pdf/493/2/2981/32846679/staa350.pdf},
}

@ARTICLE{SDSSDR18,
       author = {{Almeida}, Andr\'es and et. al },
        title = "{The Eighteenth Data Release of the Sloan Digital Sky Surveys: Targeting and First Spectra from SDSS-V}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - High Energy Astrophysical Phenomena},
         year = {2023},
        month = {1},
          eid = {arXiv:2301.07688},
        pages = {arXiv:2301.07688},
          doi = {10.48550/arXiv.2301.07688},
       eprint = {2301.07688},
 primaryClass = {astro-ph.GA},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System},
    url = {\url{https://doi.org/10.48550/arXiv.2301.07688}}
}

@article{SVMCortes1995,
    author = {Cortes, Corinna and Vapnik, Vladimir},
    title = "{Support-vector networks}",
    journal = {Machine Learning},
    year = {1995},
    month = {9},
    volume = {20},
    pages = {273-297},
    doi = {10.1007/BF00994018}
}

@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
  url = {\url{https://doi.org/10.48550/arXiv.1309.0238}}
}

@ARTICLE{kNNCover1967,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  number={1},
  pages={21-27},
  doi={10.1109/TIT.1967.1053964}}

@misc{knndiagram,
  author = {Latysheva, Natalia},
  title = {Implementing your own K-nearest neighbor algorithm using Python},
  howpublished = {\url{https://www.kdnuggets.com/2016/01/implementing-your-own-knn-using-python.html}},
  year = {2016},
  note = {Accessed: 15 June 2023},
}

@INPROCEEDINGS{RandomForest1995,
  author={Tin Kam Ho},
  booktitle={Proceedings of 3rd International Conference on Document Analysis and Recognition}, 
  title={Random decision forests}, 
  year={1995},
  volume={1},
  number={},
  pages={278-282 vol.1},
  doi={10.1109/ICDAR.1995.598994}
}


@article{Rokach2010,
	abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
	author = {Rokach, Lior},
	date = {2010/02/01},
	date-added = {2023-08-03 18:01:26 +0100},
	date-modified = {2023-08-03 18:01:26 +0100},
	doi = {10.1007/s10462-009-9124-7},
	id = {Rokach2010},
	isbn = {1573-7462},
	journal = {Artificial Intelligence Review},
	number = {1},
	pages = {1--39},
	title = {Ensemble-based classifiers},
	url = {https://doi.org/10.1007/s10462-009-9124-7},
	volume = {33},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s10462-009-9124-7}}


@article{Breiman2001,
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	author = {Breiman, Leo},
	date = {2001/10/01},
	date-added = {2023-08-04 17:03:42 +0100},
	date-modified = {2023-08-04 17:03:42 +0100},
	doi = {10.1023/A:1010933404324},
	id = {Breiman2001},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {5--32},
	title = {Random Forests},
	url = {\url{https://doi.org/10.1023/A:1010933404324}},
	volume = {45},
	year = {2001},
}

@article{Clarke2020,
   abstract = {<p> We used 3.1 million spectroscopically labelled sources from the Sloan Digital Sky Survey (SDSS) to train an optimised random forest classifier using photometry from the SDSS and the Widefield Infrared Survey Explorer. We applied this machine learning model to 111 million previously unlabelled sources from the SDSS photometric catalogue which did not have existing spectroscopic observations. Our new catalogue contains 50.4 million galaxies, 2.1 million quasars, and 58.8 million stars. We provide individual classification probabilities for each source, with 6.7 million galaxies (13%), 0.33 million quasars (15%), and 41.3 million stars (70%) having classification probabilities greater than 0.99; and 35.1 million galaxies (70%), 0.72 million quasars (34%), and 54.7 million stars (93%) having classification probabilities greater than 0.9. Precision, Recall, and <italic>F</italic> <sub>1</sub> score were determined as a function of selected features and magnitude error. We investigate the effect of class imbalance on our machine learning model and discuss the implications of transfer learning for populations of sources at fainter magnitudes than the training set. We used a non-linear dimension reduction technique, Uniform Manifold Approximation and Projection, in unsupervised, semi-supervised, and fully-supervised schemes to visualise the separation of galaxies, quasars, and stars in a two-dimensional space. When applying this algorithm to the 111 million sources without spectra, it is in strong agreement with the class labels applied by our random forest model. </p>},
   author = {A. O. Clarke and A. M. M. Scaife and R. Greenhalgh and V. Griguta},
   url = {\url{https://doi.org/10.1051/0004-6361/201936770}},
   issn = {0004-6361},
   journal = {Astronomy \& Astrophysics},
   month = {7},
   pages = {A84},
   title = {Identifying galaxies, quasars, and stars with machine learning: A new catalogue of classifications for 111 million SDSS sources without spectra},
   volume = {639},
   year = {2020},
}

@incollection{hastie-ch10,
  added-at = {2010-06-03T15:15:09.000+0200},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  edition = {2},
  publisher = {Springer},
  timestamp = {2010-06-03T15:15:09.000+0200},
  title = {Chapter 10: Boosting and Additive Trees},
  booktitle = {The elements of statistical learning: data mining, inference and prediction},
  year = {2009},
  pages = {337--387}
}

@incollection{hastie-ch7,
  added-at = {2010-06-03T15:15:09.000+0200},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  edition = {2},
  publisher = {Springer},
  timestamp = {2010-06-03T15:15:09.000+0200},
  title = {Chapter 7: Model Assessment and Selection},
  booktitle = {The elements of statistical learning: data mining, inference and prediction},
  year = {2009},
  pages = {219--259}
}

@article{Friedman2001,
 ISSN = {00905364},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 urldate = {2023-08-06},
 volume = {29},
 year = {2001}
}

@misc{singhal2020a,
  author = "G. Singhal",
  title = "Ensemble Methods in Machine Learning: Bagging Versus Boosting",
  year = {2020},
  howpublished = "\url{https://www.pluralsight.com/guides/ensemble-methods:-bagging-versus-boosting}",
  note = "Accessed: 06 August 2023"
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011},
 url={\url{https://doi.org/10.48550/arXiv.1201.0490}}
}

@article{gbcdiagram,
author = {Deng, Haowen and Zhou, Youyou and Wang, Lin and Zhang, Cheng},
year = {2021},
month = {12},
pages = {},
title = {Ensemble learning for the early prediction of neonatal jaundice with genetic features},
volume = {21},
journal = {BMC Medical Informatics and Decision Making},
url = {\url{https://doi.org/10.1186/s12911-021-01701-9}}
}

@book{hosmerlogisticregression,
  author = {Hosmer, David W. and Lemeshow, Stanley},
  biburl = {https://www.bibsonomy.org/bibtex/253e64bf95d689444a124ef2c01d00d81/sveng},
  interhash = {a10a0c6d7b6f7baf7ec6c994c031c5a3},
  intrahash = {53e64bf95d689444a124ef2c01d00d81},
  isbn = {0471356328, 9780471356325},
  keywords = {imported statistic},
  publisher = {John Wiley and Sons},
  timestamp = {2017-04-01T10:44:00.000+0200},
  title = {Applied logistic regression},
  year = {2000},
  edition = {2}
}

@misc{scikit-learn-crossval,
  title = "Cross-validation",
  author = "{Scikit-learn contributors}",
  howpublished = {\url{https://scikit-learn.org/stable/modules/cross_validation.html}},
  note = "[scikit-learn 1.3.0 documentation; Online; accessed 10 August 2023]",
  year = {2023}
}

@article{Hawkins2004,
author = {Hawkins, Douglas M.},
title = {The Problem of Overfitting},
journal = {Journal of Chemical Information and Computer Sciences},
volume = {44},
number = {1},
pages = {1-12},
year = {2004},
URL = { \url{https://doi.org/10.1021/ci0342472}},

}

@misc{fitting,
	author = {fast.ai contributors},
	title = {{O}verfitting and {U}nderfitting --- fastaireference.com},
	howpublished = {\url{https://www.fastaireference.com/overfitting}},
	year = {2023},
	note = {[Accessed 10-08-2023]},
}

@article{Tharwat2021,
   author = {Alaa Tharwat},
   url = {\url{https://doi.org/10.1016/j.aci.2018.08.003}},
   issn = {2634-1964},
   issue = {1},
   journal = {Applied Computing and Informatics},
   month = {1},
   pages = {168-192},
   title = {Classification assessment methods},
   volume = {17},
   year = {2021},
}

@article{Powers2008,
author = {Powers, David},
year = {2011},
month = {01},
pages = {37-63},
title = {Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness \& Correlation},
volume = {2},
issue = {1},
journal = {International Journal of Machine Learning Technology},
publisher = {Bioinfo Publications},
url = {\url{https://doi.org/10.48550/arXiv.2010.16061}}
}

@misc{SkyserverSDSS,
	author = {SDSS.org},
	title = {{S}{Q}{L} {S}earch - {S}kyserver{S}{D}{S}{S} -- skyserver.sdss.org},
	howpublished = {\url{https://skyserver.sdss.org/dr18/SearchTools/sql}},
	year = {2022},
	note = {[Accessed 10-08-2023]},
}

@Article{NUMPY,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = {09},
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {\url{https://doi.org/10.1038/s41586-020-2649-2}}
}

@InProceedings{PANDAS,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  url       = {\url{http://doi.org/10.25080/Majora-92bf1922-00a}}
}

@Article{MATPLOTLIB,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {\url{http://doi.org/10.1109/MCSE.2007.55}},
  year      = {2007}
}

@misc{Mendeley,
	author = {Victor Hennings},
	title = {{V}ictor {H}enning's brief guide to {M}endeley -- elsevier.com},
	howpublished = {\url{https://www.elsevier.com/connect/archive/victor-hennings-brief-guide-to-mendeley}},
	year = {2013},
	note = {[Accessed 10-08-2023]},
}

@misc{towardsdatascienceStellarClassification,
	author = {Saiffudin, Mohammed},
	title = {{S}tellar {C}lassification: {A} {M}achine {L}earning {A}pproach --towardsdatascience.com},
	howpublished = {\url{https://towardsdatascience.com/stellar-classification-a-machine-learning-approach-5e23eb5cadb1}},
	year = {2022},
	note = {[Accessed 12-08-2023]},
}

@misc{sloanSloanDigital,
	author = {},
	title = {{S}loan {D}igital {S}ky {S}urvey | {A}lfred {P}. {S}loan {F}oundation},
	howpublished = {\url{https://sloan.org/programs/research/sloan-digital-sky-survey}},
	year = {},
	note = {[Accessed 13-08-2023]},
}

@misc{github,
	author = {Pratik Barve},
	title = {{G}it{H}ub - iamstarstuff/{M}{S}c{D}ata{S}cience{T}hesis},
	howpublished = {\url{https://github.com/iamstarstuff/MScDataScienceThesis}},
	year = {2023},
	note = {[Accessed 14-08-2023]},
}